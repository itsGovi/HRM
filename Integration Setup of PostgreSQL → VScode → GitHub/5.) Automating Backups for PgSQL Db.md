# **Automating Backups for Your PostgreSQL Database**

*Think of backups as snapshots in time. They give you peace of mind by allowing you to restore your database to a previous state if something goes wrong. In this section, we’ll set up a script that automates database backups on a regular schedule, ensuring that you always have a recent copy of your data.*

___

### **1. Why Database Backups Matter**

Before diving into the technical details, let’s talk briefly about why backups are crucial:

- **Data Loss Prevention**: Mistakes happen. A backup can restore data if records are accidentally deleted or modified.
- **System Failures**: Hardware or software can fail unexpectedly. Backups minimize downtime by allowing quick recovery.
- **Testing and Debugging**: Backups create a safe testing environment where you can experiment with different database changes.

By automating backups, you save yourself from the headache of remembering to do it manually, plus you can set a schedule that fits your data’s needs.

___

### **2. Setting Up a Backup Script**

We’ll create a Python script that connects to your PostgreSQL database and saves a backup file at scheduled intervals. Here’s how to do it step-by-step:

1. **Create a New Python Script for Backups**:
    - Create a file in your project folder, e.g., `backup_script.py`.

2. **Install `subprocess`**:

    - This module helps run PostgreSQL’s command-line tools from Python. You should have `subprocess` by default, but double-check by running:

        ```bash
                pip install subprocess
        ```

3. **Write the Backup Script**:

    - Open `backup_script.py` and add the following code:

        ```python
                import subprocess
                from datetime import datetime
                import os

                # Define your PostgreSQL database details
                DB_NAME = "your_database_name"
                USER = "your_username"
                PASSWORD = "your_password"  # Use environment variables for security in real projects
                HOST = "localhost"
                BACKUP_DIR = "./backups"  # Folder to store backup files

                # Ensure the backup directory exists
                if not os.path.exists(BACKUP_DIR):
                    os.makedirs(BACKUP_DIR)

                # Get current timestamp for unique backup file name
                timestamp = datetime.now().strftime("%Y%m%d_%H%M%S")
                backup_filename = f"{BACKUP_DIR}/backup_{timestamp}.sql"

                # Construct the backup command
                command = f"pg_dump -U {USER} -h {HOST} {DB_NAME} > {backup_filename}"

                # Run the backup command
                try:
                    subprocess.run(command, shell=True, check=True, env={"PGPASSWORD": PASSWORD})
                    print(f"Backup successful! Saved to {backup_filename}")
                except subprocess.CalledProcessError as e:
                    print(f"Backup failed: {e}")
        ```

    - **Explanation**:

    - We define our database details, like `DB_NAME`, `USER`, and `PASSWORD`.
    - The `timestamp` variable ensures each backup has a unique filename.
    - The `pg_dump` command performs the actual backup, saving it as an `.sql` file in the `backups` folder.
    - **Note**: For security, it’s better to store sensitive information (like your database password) as environment variables rather than directly in your code.

4. **Test the Backup Script**:

    - Run the script in your terminal to make sure it works:

        ```bash
                python backup_script.py
        ```

    - If successful, you should see a message indicating the backup location. The `.sql` file will contain a complete snapshot of your database.

___

### **3. Automating the Backup with Task Scheduler (Windows) or Cron (Mac/Linux)**

To automate backups, let’s set up a schedule so this script runs regularly without you having to remember.

#### **For Windows Users: Using Task Scheduler**

1. **Open Task Scheduler**:
   - Search for Task Scheduler in your Windows search bar and open it.

2. **Create a New Task**:
   - Click **Action > Create Task**.
   - Name it something like “PostgreSQL Backup.”

3. **Set the Trigger**:
   - Go to the **Triggers** tab, then click **New**.
   - Set it to run daily or at whatever frequency fits your needs.

4. **Set the Action**:
   - Go to the **Actions** tab, then click **New**.
   - Set **Action** to “Start a Program,” then browse to your Python executable.
   - Add the path to `backup_script.py` as an argument.

5. **Save and Test**:
   - Click **OK** to save, then test the task by running it manually to see if it performs the backup.

### **For Mac/Linux Users: Using Cron**

1. **Open the Cron Editor**:
   - In the terminal, type:

     ```bash
            crontab -e
     ```

2. **Add a New Cron Job**:
   - Add a line that schedules your backup script. For example, to run it daily at 2 a.m., add:

     ```bash
     0 2 * * * /usr/bin/python3 /path/to/backup_script.py
     ```

   - Adjust the timing as needed. The path to `python3` and `backup_script.py` will depend on your setup.

3. **Save and Exit**:
   - Save the cron job, and it will run at the scheduled time. You can check logs to verify it’s working correctly.

___

### **4. Testing and Monitoring Backups**

Once the automation is set up, test it to ensure it runs without issues:

- **Run the Task Manually**: Trigger it manually and check that a new backup file is created.
- **Check Backup Files Regularly**: Open the backup files occasionally to ensure they contain data, especially after significant database changes.
- **Set Up Alerts** (Optional): If you want, consider setting up an email notification if a backup fails. This requires more configuration, but it’s a valuable addition for critical systems.

___

## **Conclusion: Protecting Your Data**

Congratulations! You now have automated backups in place, providing you with data security and peace of mind. This setup ensures that even if something goes wrong with your main database, you’ll have recent copies available for restoration.

___


***[OR]***


___

Here’s a version of **Automating Backups for PostgreSQL Database** that integrates your new weekly incremental backup approach. This revised section outlines how the automated backup uses `incremental_backup.sh` to create weekly `.sql` files that log only modified records, making it both memory-efficient and easy to review by date.

___

To protect your data and keep track of changes, we’ll automate a weekly backup process that only saves modified records. This reduces memory usage and creates organized `.sql` files for each week (Monday-Sunday), allowing you to see exactly what changed on specific dates.

### Overview

The backup setup includes:

- **Incremental Backups**: Only records modified since the last backup are saved.
- **Weekly Files**: Files are generated each week, named by date range (e.g., `2024-10-28_to_2024-11-03.sql`).
- **Daily Entries**: Each day’s changes within the week are appended, simplifying tracking changes over time.

### How It Works

The setup uses the following files:

1. **`database.py`** – Establishes the database connection.
2. **`pre-commit` Git Hook** – Triggers the `incremental_backup.sh` script.
3. **`incremental_backup.sh`** – Backs up modified data by table and date.

### Instructions

#### Step 1: Database Connection Setup

In `database.py`, ensure the connection to your PostgreSQL database is correctly configured:

```python
# database.py
import os
from sqlalchemy import create_engine
from dotenv import load_dotenv

# Load environment variables from .env file
load_dotenv()

# Retrieve database connection info from environment variables
DB_NAME = os.getenv("DB_NAME")
DB_USER = os.getenv("DB_USER")
DB_PASSWORD = os.getenv("DB_PASSWORD")
DB_HOST = "localhost"
DB_PORT = "5432"

# Construct the database URL
DATABASE_URL = f"postgresql://{DB_USER}:{DB_PASSWORD}@{DB_HOST}:{DB_PORT}/{DB_NAME}"

# Create a database engine
engine = create_engine(DATABASE_URL)

if __name__ == "__main__":
    print("Database connected successfully!")
```

Ensure that the environment variables in `.env` (e.g., `DB_NAME`, `DB_USER`, `DB_PASSWORD`) match your database details.

#### Step 2: Pre-commit Hook for Automated Backup

The `pre-commit` hook will run the backup script before each commit. This ensures backups are up-to-date with changes.

Add the following to `.git/hooks/pre-commit`:

```bash
#!/bin/bash
./sql/backup_scripts/incremental_backup.sh
```

This hook will execute the backup script automatically before each commit, creating or updating the weekly `.sql` backup file in `data/backup_records`.

#### Step 3: Incremental Backup Script

The core of this backup system is the `incremental_backup.sh` script, which:

- Creates a new `.sql` file for the week if it doesn’t already exist.
- Adds daily entries for modified records only.
- Saves weekly backups in `data/backup_records`.

Place this script at `sql/backup_scripts/incremental_backup.sh`:

```bash
#!/bin/bash

# Configuration
export PGPASSWORD='your_password'
BACKUP_DIR="./data/backup_records"
DB_NAME="hr_resource_db"
DB_USER="postgres"

# Create backup directory if it doesn't exist
mkdir -p "$BACKUP_DIR"

# Calculate the weekly date range (Monday to Sunday)
WEEK_START=$(date -d 'last-monday' +%Y-%m-%d)
WEEK_END=$(date -d "$WEEK_START +6 days" +%Y-%m-%d)
FILE_NAME="${WEEK_START}_to_${WEEK_END}.sql"
CURRENT_DATE=$(date '+%Y-%m-%d')

# Retrieve last backup date from the saved file
get_last_backup_date() {
    if [ -f "$BACKUP_DIR/last_backup_date" ]; then
        cat "$BACKUP_DIR/last_backup_date"
    else
        echo "1970-01-01 00:00:00"
    fi
}

LAST_BACKUP=$(get_last_backup_date)

# Add today's header if not already in the file
if ! grep -q "### Changes for $CURRENT_DATE ###" "$BACKUP_DIR/$FILE_NAME"; then
    echo -e "\n\n### Changes for $CURRENT_DATE ###" >> "$BACKUP_DIR/$FILE_NAME"
fi

# Function to backup modified data from a table
backup_modified_table() {
    local table=$1
    echo "-- Modified records in $table on $CURRENT_DATE" >> "$BACKUP_DIR/$FILE_NAME"
    pg_dump -U "$DB_USER" -d "$DB_NAME" --data-only --table="$table" --where="updated_at > '$LAST_BACKUP'" >> "$BACKUP_DIR/$FILE_NAME"
}

# List of tables to back up incrementally
tables=("departments" "positions" "employees" "skills" "employee_skills" "leave_requests")

# Backup modified data from each table
for table in "${tables[@]}"; do
    backup_modified_table "$table"
done

# Finalize the transaction block
echo "COMMIT;" >> "$BACKUP_DIR/$FILE_NAME"

# Update last backup date
date '+%Y-%m-%d %H:%M:%S' > "$BACKUP_DIR/last_backup_date"

# Clear PGPASSWORD for security
unset PGPASSWORD
echo "Incremental backup completed for $CURRENT_DATE and saved in $FILE_NAME."
```

### Schedule and Verify Backups

1. **Weekly Schedule**: The `pre-commit` hook ensures that backups run frequently before each commit.
2. **Verify Backup Files**: Check the `data/backup_records` directory for `.sql` files labeled by week (e.g., `2024-10-28_to_2024-11-03.sql`).
3. **Inspect Daily Changes**: Each day’s updates within the week will appear under headers like `### Changes for 2024-10-30 ###`.

This automated system offers efficient, organized backups with minimal storage overhead, allowing you to track daily changes within weekly backups.
